### 指示
以下に入力した文章を要約してください。
ただし、初心者でも分かりやすい日本語で出力してください。
### 入力
A large-scale language model is a computer language model consisting of an artificial neural network with a large number of parameters (tens of millions to billions), trained by self-supervised or semi-supervised learning using a huge amount of unlabeled text. LLMs emerged around 2018 and have demonstrated excellent performance in a variety of tasks. This has shifted the focus of natural language processing research from the previous paradigm of training supervised models specialized for specific tasks. Although the application of large-scale language models has achieved remarkable results, the development of large-scale language models is still in its infancy, and many researchers are contributing to the improvement of large-scale language models. There is no formal definition of the term large-scale language model, but it often refers to deep learning models with millions to billions or more parameters that are pre-trained on large corpora. Unlike those trained for a specific task (sentiment analysis, named entity extraction, mathematical reasoning, etc.), LLMs are general-purpose models that excel at a wide range of tasks. In a sense, the ability of LLMs to perform tasks and the scope they can address seem to be a function of the amount of resources (data, parameter size, computational power) devoted to LLMs, rather than relying on breakthroughs in design. It turns out that neural language models with many parameters can capture much of the syntax and meaning of human language when sufficiently trained on the simple task of predicting the next word in a sentence. Moreover, large language models show considerable general knowledge about the world and can "memorize" large amounts of facts during training. According to a 2023 meta-analysis, which is considered high-quality evidence, there are of course researchers all over the world who are excited by the creativity of large language models, and some scholars claim that large language models are creative in tasks that small language models cannot do, but it is suggested that this is due to the choice of metric, not creativity. It is suggested that the creativity advantage of large language models may not be observed if a different metric is chosen.
