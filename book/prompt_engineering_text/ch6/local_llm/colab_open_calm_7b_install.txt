# 環境のインストール --- (*1)
!pip install -U \
  transformers==4.30.2 \
  sentencepiece==0.1.99 \
  accelerate==0.20.3

# モデルを指定 --- (*2)
model_name = "cyberagent/open-calm-7b" # メモリが許す場合こちらがオススメ
# model_name = "cyberagent/open-calm-medium" # メモリエラーになる場合はこちらを

# モデルの読み込み --- (*3)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 手軽にモデルを実行できるように関数を定義 --- (*4)
def generate(prompt, temperature=0.7, max_tokens=100):
  prompt = prompt.strip()
  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
  with torch.no_grad():
      tokens = model.generate(
          **inputs,
          max_new_tokens=max_tokens,
          do_sample=True,
          temperature=temperature,
          top_p=0.9,
          repetition_penalty=1.05,
          pad_token_id=tokenizer.pad_token_id,
      )
  output = tokenizer.decode(tokens[0], skip_special_tokens=True)
  return output
