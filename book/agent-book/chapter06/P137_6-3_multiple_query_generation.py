"""
Title   : LangChainã¨LangGraphã«ã‚ˆã‚‹RAGãƒ»AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè·µå…¥é–€
Chapter : 6 Advanced RAG
Section : 3 è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆ
Theme   : è¤‡æ•°ã®ä»®å›ç­”ã«åŸºã¥ã„ã¦HyDEã‚’è¡Œã†
Date    : 2025/06/09
Page    : P134-138
"""

# ï¼œæ¦‚è¦ï¼
# - ä»®èª¬çš„ãªå›ç­”ã‚’è¤‡æ•°ä½œæˆã—ã¦é©åˆ‡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå«ã¾ã‚Œã‚„ã™ãã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚‹
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã“ã¨ã§å®‰å®šæ€§ãŒæœŸå¾…ã§ãã‚‹

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from pydantic import BaseModel, Field


# RAGãƒ‘ãƒ¼ãƒ„ã®æº–å‚™ ----------------------------------------

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# DBæ§‹ç¯‰
persist_directory = "./chroma_langchain"
db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

# ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼å®šç¾©
retriever = db.as_retriever()


# å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã®æº–å‚™
# --- ä»®èª¬çš„ãªå›ç­”ã‚’æ ¼ç´
class QueryGenerationOutput(BaseModel):
    queries: list[str] = Field(..., description="æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ")


# LLMãƒ‘ãƒ¼ãƒ„ã®å®šç¾© ----------------------------------------

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = ChatPromptTemplate.from_template('''\
ä»¥ä¸‹ã®æ–‡è„ˆã ã‘ã‚’è¸ã¾ãˆã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æ–‡è„ˆ: """
{context}
"""

è³ªå•: {question}
''')


# LLMãƒ¢ãƒ‡ãƒ«
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼
output_parser = StrOutputParser()


# é–¢æ•°å®šç¾©ï¼šãƒ­ã‚°ç¢ºèª --------------------------------------


def log_queries(queries):
    print("ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆmulti-queryï¼‰:")
    for i, q in enumerate(queries):
        print(f"  [{i + 1}] {q}")
    return queries


def log_docs(nested_docs):
    print("ğŸ” æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ï¼ˆã‚¯ã‚¨ãƒªã”ã¨ï¼‰:")
    for query_idx, docs in enumerate(nested_docs):
        print(f"ã‚¯ã‚¨ãƒª {query_idx + 1} ã«å¯¾ã™ã‚‹æ¤œç´¢çµæœ:")
        for doc_idx, doc in enumerate(docs):
            content = getattr(doc, "page_content", str(doc))  # å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            print(f"  --- Doc {doc_idx + 1} ---\n{content[:200]}...\n")
    return nested_docs


# ã‚¯ã‚¨ãƒªç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ ----------------------------

# ï¼œãƒã‚¤ãƒ³ãƒˆï¼
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹
# - ä½¿ç”¨æ„å›³ã‚’ä¼ãˆã‚‹ã“ã¨ã§é©åˆ‡ãªå›ç­”ã‚’å¾—ã‚„ã™ãã™ã‚‹

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
query_generation_prompt = ChatPromptTemplate.from_template(
    template="""\
    è³ªå•ã«å¯¾ã—ã¦ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã«ã€
    3ã¤ã®ç•°ãªã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
    è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§æ¤œç´¢ã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦è¤‡æ•°ã®è¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚

    è³ªå•: {question}
"""
)

# ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
query_generation_chain = (
    query_generation_prompt
    | model.with_structured_output(schema=QueryGenerationOutput)
    | (lambda x: x.queries)
    | RunnableLambda(log_queries)
)

# ä»®èª¬çš„ãªå›ç­”ã‚’ãƒ©ãƒƒãƒ—ã—ãŸChainã‚’å®šç¾© ----------------------

multi_query_rag_chain = (
    {
        "question": RunnablePassthrough(),
        "context": query_generation_chain | retriever.map() | RunnableLambda(log_docs),
    }
    | prompt
    | model
    | output_parser
)


# å•ã„åˆã‚ã›
multi_query_rag_chain.invoke(input="LangChainã®æ¦‚è¦ã‚’æ•™ãˆã¦")
